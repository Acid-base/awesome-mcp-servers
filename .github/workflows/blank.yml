---
name: MCP Server Awesome List Tests (Synchronous)
on:
  workflow_dispatch: null
  schedule:
    - cron: 0 5 * * 1
jobs:
  test-all-servers:
    name: Test All MCP Servers Sequentially
    runs-on: ubuntu-latest
    timeout-minutes: 360
    steps:
      - name: Setup Prerequisites (Node, uv, Go, Deno, jq)
        shell: bash -eo pipefail {0}
        run: >
          echo "Setting up Node.js, uv, Go, Deno, jq..."

          sudo apt-get update

          sudo apt-get install -y --no-install-recommends ca-certificates curl gnupg git coreutils jq

          echo "Installed base packages (including jq)."

          echo "Setting up Node.js v20..."

          curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash -

          sudo apt-get install -y nodejs

          echo "Node Version: $(node --version)"

          echo "NPM Version: $(npm --version)"

          echo "Setting up uv..."

          curl -LsSf https://astral.sh/uv/install.sh | sh

          export PATH="$HOME/.cargo/bin:$PATH"

          echo "uv Version: $(uv --version)"

          echo "Setting up Go..."

          GO_VERSION="1.22.2" # Specify a recent version

          curl -fsSL "https://golang.org/dl/go${GO_VERSION}.linux-amd64.tar.gz" -o go.tar.gz

          sudo rm -rf /usr/local/go && sudo tar -C /usr/local -xzf go.tar.gz

          rm go.tar.gz

          export PATH="/usr/local/go/bin:$PATH"

          echo "Go Version: $(go version)"

          echo "Setting up Deno..."

          curl -fsSL https://deno.land/install.sh | sh

          export DENO_INSTALL="$HOME/.deno"

          export PATH="$DENO_INSTALL/bin:$PATH"

          echo "Deno Version: $(deno --version)"

          echo "Verifying Docker..."

          docker --version

          echo "Verifying timeout command..."

          command -v timeout

          echo "Verifying tr command..."

          command -v tr

          echo "Verifying jq command..."

          command -v jq

          echo "Exporting PATH for subsequent steps..."

          FINAL_PATH="$HOME/.cargo/bin:$HOME/.deno/bin:/usr/local/go/bin:${PATH}"

          echo "Final PATH to be exported: $FINAL_PATH"

          echo "PATH=$FINAL_PATH" >> $GITHUB_ENV
      - name: Fetch and Parse Server List
        id: fetch
        shell: bash -eo pipefail {0}
        run: >
          echo "Checking out fork to get server list..."

          git clone --depth 1 https://github.com/Acid-base/awesome-mcp-servers.git awesome-list-fork

          cd awesome-list-fork

          echo "Parsing README.md..."

          awk '/^## Server Implementations/{flag=1; next} /^## Frameworks/{flag=0} flag' README.md | \
            grep -oE 'https?:\/\/(github\.com|gitlab\.com|gitea\.com)\/[^/]+\/[^)/ ]+' | \
            sed -E 's/^https?:\/\///' | \
            sed 's/\.git$//' | \
            sort -u > ../repo_list.txt
          REPO_COUNT=$(cat ../repo_list.txt | wc -l)

          echo "Found $REPO_COUNT potential servers."

          if [ "$REPO_COUNT" -eq 0 ]; then
            echo "::warning::No server implementations found in README.md."
          fi

          echo "count=$REPO_COUNT" >> $GITHUB_OUTPUT

          echo "Server List (repo_list.txt):"

          cat ../repo_list.txt

          cd ..

          rm -rf awesome-list-fork
      - name: Test Servers Sequentially
        id: test-servers-sequentially
        env:
          GH_MCP_TOKEN: ${{ secrets.MCP_TESTER_GITHUB_PAT }}
        shell: bash -eo pipefail {0}
        run: >
          REPO_LIST_FILE="repo_list.txt"

          if [ ! -f "$REPO_LIST_FILE" ]; then
              echo "::error::Server list file ($REPO_LIST_FILE) not found!"
              exit 1
          fi


          REPO_COUNT=${{ steps.fetch.outputs.count }}

          if [ -z "$REPO_COUNT" ] || [ "$REPO_COUNT" -eq 0 ]; then
              echo "::notice::No servers found in the list file. Exiting job gracefully."
              echo "[]" > detailed_results.json # Create empty results file for artifact
              exit 0
          fi


          CURRENT_INDEX=0

          SUCCESS_COUNT=0

          FAILURE_COUNT=0

          SKIPPED_COUNT=0

          declare -a RESULTS # Array to hold detailed JSON results


          echo "Starting server tests. Using PATH: $PATH"


          while IFS= read -r REPO || [ -n "$REPO" ]; do
            if [ -z "$REPO" ]; then continue; fi
            REPO=$(echo "$REPO" | sed 's:/*$::')
            CURRENT_INDEX=$((CURRENT_INDEX + 1))
            echo ""
            echo "======================================================================"
            command -v tr >/dev/null 2>&1 || { echo "::error:: 'tr' command not found in PATH ($PATH)!"; exit 1; }
            REPO_SLUG=$(echo "$REPO" | tr '/' '_' | tr '.' '_')
            echo "Starting Test [$CURRENT_INDEX/$REPO_COUNT]: ${REPO} (Slug: ${REPO_SLUG})"
            echo "======================================================================"
            echo ""

            # --- Variables ---
            CHECKOUT_DIR="${GITHUB_WORKSPACE}/mcp_server_under_test_${CURRENT_INDEX}"
            STATUS="SKIPPED"; REASON="Test logic did not complete."; RUN_METHOD="None"
            TEST_START_TIME=$(date +%s); CHECKOUT_PATH="$CHECKOUT_DIR"

            # --- A. Checkout ---
            echo "--> Checking out $REPO into $CHECKOUT_PATH..."
            rm -rf "$CHECKOUT_PATH"
            GIT_CLONE_URL=""
            if [[ "$REPO" == */* && ! "$REPO" == *.*/* ]]; then GIT_CLONE_URL="https://github.com/${REPO}.git"
            elif [[ "$REPO" == github.com/* || "$REPO" == gitlab.com/* || "$REPO" == gitea.com/* ]]; then GIT_CLONE_URL="https://${REPO}.git"
            else echo "::warning:: Unrecognized repo format: ${REPO}. Assuming GitHub."; GIT_CLONE_URL="https://github.com/${REPO}.git"; fi
            echo "--> Cloning URL: $GIT_CLONE_URL"
            timeout 300s git clone --depth 1 "$GIT_CLONE_URL" "$CHECKOUT_PATH"; CHECKOUT_SUCCESS=$?

            if [ $CHECKOUT_SUCCESS -ne 0 ]; then REASON="Checkout failed (Exit code $CHECKOUT_SUCCESS). URL: $GIT_CLONE_URL"; STATUS="SKIPPED"
            elif [ ! -d "$CHECKOUT_PATH" ] || [ -z "$(ls -A "$CHECKOUT_PATH")" ]; then REASON="Checkout succeeded but directory is empty. URL: $GIT_CLONE_URL"; STATUS="SKIPPED"
            else
               echo "--> Checkout successful."
               # --- B. Test Logic (Subshell) ---
               SUBSHELL_LOG_FILE="${GITHUB_WORKSPACE}/subshell_${REPO_SLUG}.log"
               SUBSHELL_RESULT_FILE="${GITHUB_WORKSPACE}/subshell_${REPO_SLUG}.result"
               ( # Start subshell
                 cd "$CHECKOUT_PATH"; set -e # Change dir, exit on error
                 # Init subshell vars
                 SUB_STATUS="SKIPPED"; SUB_REASON="No applicable test method found."; SUB_RUN_METHOD="None"
                 # Trap to write results on exit (normal or error)
                 write_result() { echo "SUB_STATUS=${SUB_STATUS:-SKIPPED}" > "$SUBSHELL_RESULT_FILE"; echo "SUB_REASON=${SUB_REASON:-Subshell exited unexpectedly}" >> "$SUBSHELL_RESULT_FILE"; echo "SUB_RUN_METHOD=${SUB_RUN_METHOD:-None}" >> "$SUBSHELL_RESULT_FILE"; }
                 trap write_result EXIT
                 # Start detection
                 echo "--- Detecting Run Method (in $PWD) ---"
                 NORMALIZED_REPO_NAME=$(echo "$REPO" | sed -E 's/^(github\.com|gitlab\.com|gitea\.com)\///' | sed 's/\.git$//')
                 echo "Normalized Repo Name for matching: $NORMALIZED_REPO_NAME"

                 # ==========================================
                 # ===== TEST LOGIC FOR EACH SERVER TYPE ====
                 # ==========================================

                 # Priority 1: Known Specific Servers
                 if [[ "$NORMALIZED_REPO_NAME" == "github/github-mcp-server" ]]; then
                   SUB_RUN_METHOD="Docker Image (Official GitHub)"
                   echo "--> Matched specific: github/github-mcp-server"
                   if [ -z "$GH_MCP_TOKEN" ]; then
                     echo "::warning::Required secret MCP_TESTER_GITHUB_PAT not set."; SUB_STATUS="SKIPPED"; SUB_REASON="Required secret MCP_TESTER_GITHUB_PAT not set"; exit 0; # Exit subshell gracefully
                   fi
                   SERVER_CMD="docker run -i --rm -e GITHUB_PERSONAL_ACCESS_TOKEN=$GH_MCP_TOKEN ghcr.io/github/github-mcp-server"
                   echo "--> Testing command: npx inspector --cli \"<docker cmd>\" --method tools/list"
                   timeout 180s npx --yes @modelcontextprotocol/inspector --cli "$SERVER_CMD" --method tools/list
                   # If timeout or npx fails, set -e causes subshell exit, trap writes last status
                   SUB_STATUS="SUCCESS"; SUB_REASON="Tested via Docker Image (stdio)"

                 elif [[ "$NORMALIZED_REPO_NAME" == "microsoft/playwright-mcp" ]]; then
                   SUB_RUN_METHOD="npx @playwright/mcp"
                   echo "--> Matched specific: microsoft/playwright-mcp"
                   SERVER_CMD="npx --yes @playwright/mcp@latest --headless"
                   echo "--> Testing command: npx inspector --cli \"$SERVER_CMD\" --method tools/list"
                   timeout 180s npx --yes @modelcontextprotocol/inspector --cli "$SERVER_CMD" --method tools/list
                   SUB_STATUS="SUCCESS"; SUB_REASON="Tested via npx (stdio)"

                 elif [[ "$NORMALIZED_REPO_NAME" == "pydantic/pydantic-ai" ]]; then
                   SUB_RUN_METHOD="deno run jsr:"
                   echo "--> Matched specific: pydantic/mcp-run-python (from pydantic-ai repo)"
                   echo "--> Running Deno warmup from ${GITHUB_WORKSPACE}..."
                   # Run warmup outside checkout dir; set -e handles failure
                   ( cd "${GITHUB_WORKSPACE}" && deno run -A --node-modules-dir=auto jsr:@pydantic/mcp-run-python warmup )
                   echo "--> Testing command: npx inspector --cli \"deno run ... stdio\" --method tools/list"
                   # Run server directly using JSR package
                   SERVER_CMD="deno run -A --node-modules-dir=auto jsr:@pydantic/mcp-run-python stdio"
                   timeout 180s npx --yes @modelcontextprotocol/inspector --cli "$SERVER_CMD" --method tools/list
                   SUB_STATUS="SUCCESS"; SUB_REASON="Tested via Deno JSR (stdio)"

                 # Priority 2: Docker Compose + Dockerfile Fallback
                 elif [ -f "docker-compose.yml" ] || [ -f "compose.yml" ]; then
                   SUB_RUN_METHOD="Docker Compose (Detected)"
                   echo "--> Found docker-compose.yml/compose.yml."
                   if [ -f "Dockerfile" ]; then
                     echo "--> Dockerfile found. Attempting Dockerfile build/run test..."
                     SUB_RUN_METHOD="Dockerfile (Compose Fallback)"
                     IMAGE_TAG="mcp-test-image-${REPO_SLUG}"
                     CONTAINER_NAME="mcp-server-test-${REPO_SLUG}"
                     echo "--> Building Docker image $IMAGE_TAG..."
                     timeout 300s docker build -t "$IMAGE_TAG" . # Build failure handled by set -e
                     HOST_PORT=6277; INTERNAL_PORT=8080 # Reset ports
                     echo "--> Starting container $CONTAINER_NAME mapping ${HOST_PORT}:${INTERNAL_PORT}..."
                     docker run -d --rm --name "$CONTAINER_NAME" -p "${HOST_PORT}:${INTERNAL_PORT}" "$IMAGE_TAG"; RUN_SUCCESS=$?
                     sleep 5 # Allow time for crash
                     # Check if container is running
                     if ! docker ps --filter "name=$CONTAINER_NAME" --filter "status=running" | grep -q "$CONTAINER_NAME"; then
                       echo "--> Initial run failed/exited (Code: $RUN_SUCCESS). Trying internal port 6277..."
                       INTERNAL_PORT=6277
                       set +e; docker stop "$CONTAINER_NAME" > /dev/null 2>&1; docker rm "$CONTAINER_NAME" > /dev/null 2>&1; set -e # Cleanup first
                       docker run -d --rm --name "$CONTAINER_NAME" -p "${HOST_PORT}:${INTERNAL_PORT}" "$IMAGE_TAG"; RUN_SUCCESS=$?
                       sleep 5 # Wait again after trying second port
                     fi
                     # Final check if container is running
                     if ! docker ps --filter "name=$CONTAINER_NAME" --filter "status=running" | grep -q "$CONTAINER_NAME"; then
                       echo "::error::Failed to start container '$CONTAINER_NAME' on ports 8080 or 6277 (Exit code hint: $RUN_SUCCESS)."; SUB_STATUS="FAILURE"; SUB_REASON="docker run failed or container exited"; exit 1; # Force subshell exit
                     fi
                     echo "--> Container running. Waiting 20s for server initialization..." && sleep 20
                     TEST_URL="http://localhost:${HOST_PORT}/sse"
                     echo "--> Testing SSE endpoint: $TEST_URL"
                     timeout 180s npx --yes @modelcontextprotocol/inspector --cli "$TEST_URL" --method tools/list
                     SUB_STATUS="SUCCESS"; SUB_REASON="Tested via Dockerfile build/run (SSE)"
                     echo "--> Stopping container ${CONTAINER_NAME}..."
                     set +e; docker stop "$CONTAINER_NAME" > /dev/null 2>&1 || true; set -e # Attempt cleanup, don't fail subshell if stop fails
                   else
                     SUB_STATUS="SKIPPED"; SUB_REASON="docker-compose.yml found, but no Dockerfile for fallback and direct compose testing not implemented."; exit 0; # Graceful exit for skip
                   fi

                 # Priority 3: Dockerfile only
                 elif [ -f "Dockerfile" ]; then
                     SUB_RUN_METHOD="Dockerfile"
                     echo "--> Found Dockerfile. Building..."
                     IMAGE_TAG="mcp-test-image-${REPO_SLUG}"
                     CONTAINER_NAME="mcp-server-test-${REPO_SLUG}"
                     timeout 300s docker build -t "$IMAGE_TAG" . # Build failure handled by set -e
                     HOST_PORT=6277; INTERNAL_PORT=8080 # Reset ports
                     echo "--> Starting container $CONTAINER_NAME mapping ${HOST_PORT}:${INTERNAL_PORT}..."
                     docker run -d --rm --name "$CONTAINER_NAME" -p "${HOST_PORT}:${INTERNAL_PORT}" "$IMAGE_TAG"; RUN_SUCCESS=$?
                     sleep 5 # Allow time for crash
                     # Check if container is running
                     if ! docker ps --filter "name=$CONTAINER_NAME" --filter "status=running" | grep -q "$CONTAINER_NAME"; then
                       echo "--> Initial run failed/exited (Code: $RUN_SUCCESS). Trying internal port 6277..."
                       INTERNAL_PORT=6277
                       set +e; docker stop "$CONTAINER_NAME" > /dev/null 2>&1; docker rm "$CONTAINER_NAME" > /dev/null 2>&1; set -e # Cleanup first
                       docker run -d --rm --name "$CONTAINER_NAME" -p "${HOST_PORT}:${INTERNAL_PORT}" "$IMAGE_TAG"; RUN_SUCCESS=$?
                       sleep 5 # Wait again after trying second port
                     fi
                     # Final check if container is running
                     if ! docker ps --filter "name=$CONTAINER_NAME" --filter "status=running" | grep -q "$CONTAINER_NAME"; then
                       echo "::error::Failed to start container '$CONTAINER_NAME' on ports 8080 or 6277 (Exit code hint: $RUN_SUCCESS)."; SUB_STATUS="FAILURE"; SUB_REASON="docker run failed or container exited"; exit 1; # Force subshell exit
                     fi
                     echo "--> Container running. Waiting 20s for server initialization..." && sleep 20
                     TEST_URL="http://localhost:${HOST_PORT}/sse"
                     echo "--> Testing SSE endpoint: $TEST_URL"
                     timeout 180s npx --yes @modelcontextprotocol/inspector --cli "$TEST_URL" --method tools/list
                     SUB_STATUS="SUCCESS"; SUB_REASON="Tested via Dockerfile build/run (SSE)"
                     echo "--> Stopping container ${CONTAINER_NAME}..."
                     set +e; docker stop "$CONTAINER_NAME" > /dev/null 2>&1 || true; set -e # Attempt cleanup, don't fail subshell if stop fails

                 # Priority 4: package.json (Node.js)
                 elif [ -f "package.json" ]; then
                   SUB_RUN_METHOD="Node.js (package.json)"
                   echo "--> Found package.json. Running npm install/ci..."
                   # Use ci if lock file exists, otherwise install
                   if [ -f "package-lock.json" ] || [ -f "npm-shrinkwrap.json" ]; then
                     echo "--> Using npm ci"
                     timeout 300s npm ci --no-audit --no-fund --loglevel=error
                   else
                     echo "--> Using npm install"
                     timeout 300s npm install --no-audit --no-fund --loglevel=error
                   fi # Install failure handled by set -e
                   # Try standard start commands
                   SERVER_CMD=""
                   INSPECTOR_SUCCESS=false
                   # Check if 'start' script exists
                   if node -e "process.exit(require('./package.json').scripts?.start ? 0 : 1)"; then
                     echo "--> Trying 'npm start'..."
                     SERVER_CMD="npm start"
                     # Try running inspector, continue if it fails (set +e temporarily or check exit code)
                     set +e # Temporarily disable exit on error for the inspector check
                     timeout 180s npx --yes @modelcontextprotocol/inspector --cli "$SERVER_CMD" --method tools/list
                     if [[ $? -eq 0 ]]; then INSPECTOR_SUCCESS=true; fi
                     set -e # Re-enable exit on error
                   else
                      echo "--> No 'start' script found in package.json."
                   fi
                   # If npm start failed or doesn't exist, try node .
                   if ! $INSPECTOR_SUCCESS ; then
                     echo "--> Trying 'node .'..."
                     SERVER_CMD="node ."
                     # Check if main entry point exists before trying node .
                     MAIN_FILE=$(node -p "try { require('./package.json').main || 'index.js' } catch(e) { 'index.js' }") # Default to index.js, handle parse errors
                     if [ -f "$MAIN_FILE" ]; then
                        set +e # Temporarily disable exit on error
                        timeout 180s npx --yes @modelcontextprotocol/inspector --cli "$SERVER_CMD" --method tools/list
                        if [[ $? -eq 0 ]]; then INSPECTOR_SUCCESS=true; fi
                        set -e # Re-enable exit on error
                     else
                        echo "--> Main entry point '$MAIN_FILE' not found, skipping 'node .' test."
                     fi
                   fi
                   # Final status based on inspector result
                   if $INSPECTOR_SUCCESS; then
                     SUB_STATUS="SUCCESS"; SUB_REASON="Tested via Node.js run (stdio using '$SERVER_CMD')";
                   else
                     echo "::error::Inspector failed with standard Node commands ('npm start', 'node .')."
                     # Check if 'start' or 'main' suggests non-stdio, skip if so
                     set +e # Need node check to not exit script if it fails
                     IS_HTTP_LIKELY=$(node -e "try{const p=require('./package.json');process.exit((p.scripts?.start&&!p.scripts.start.includes('stdio'))||(p.main&&!p.main.includes('stdio'))?0:1)}catch(e){process.exit(1)}")
                     set -e
                     if [[ $? -eq 0 ]]; then # Node check succeeded and returned 0 (likely HTTP)
                       SUB_STATUS="SKIPPED"; SUB_REASON="Failed stdio test. Project might intend non-stdio execution (HTTP?)." ; exit 0; # Graceful exit for skip
                     else # Node check failed or returned 1 (likely intended stdio or error)
                       SUB_STATUS="FAILURE"; SUB_REASON="Inspector failed with standard Node commands."; exit 1; # Fail subshell
                     fi
                   fi

                 # Priority 5: pyproject.toml / requirements.txt (Python)
                 elif [ -f "pyproject.toml" ] || [ -f "requirements.txt" ]; then
                   SUB_RUN_METHOD="Python (pyproject/requirements)"
                   echo "--> Found Python project. Setting up environment with uv..."
                   uv venv .venv --seed # Error handled by set -e
                   source .venv/bin/activate # Error handled by set -e
                   echo "--> Running uv pip install/sync..."
                   # Determine install method based on available files
                   if [ -f "pyproject.toml" ] && grep -qE '^\[project\]' pyproject.toml; then
                      echo "--> Found [project] table, running 'uv pip install .[all]'" # Try installing optional dependencies too
                      # Try with [all], fallback to plain install
                      set +e # Allow first install attempt to fail
                      timeout 300s uv pip install ".[all]"
                      INSTALL_SUCCESS=$?
                      set -e
                      if [[ $INSTALL_SUCCESS -ne 0 ]]; then
                         echo "--> 'uv pip install .[all]' failed, trying 'uv pip install .'"
                         timeout 300s uv pip install "." # Error handled by set -e
                      fi
                   elif [ -f "requirements.lock" ] || [ -f "uv.lock" ]; then
                      echo "--> Found lock file, running 'uv sync --locked'"
                      timeout 300s uv sync --locked || timeout 300s uv sync # Fallback if locked fails
                   elif [ -f "requirements.txt" ]; then
                      echo "--> Found requirements.txt, running 'uv pip install -r requirements.txt'"
                      timeout 300s uv pip install -r requirements.txt
                   else # Only pyproject.toml without [project]
                      echo "--> Found pyproject.toml only (no [project]), running 'uv sync'"
                      timeout 300s uv sync # Best guess, error handled by set -e
                   fi
                   # Try standard execution methods
                   SERVER_CMD=""
                   INSPECTOR_SUCCESS=false
                   PYTHON_EXEC="python" # Use python from venv
                   # Try common entry points
                   if [ -f "main.py" ]; then
                     echo "--> Trying '$PYTHON_EXEC main.py'..."
                     SERVER_CMD="$PYTHON_EXEC main.py"
                     set +e; timeout 180s npx --yes @modelcontextprotocol/inspector --cli "$SERVER_CMD" --method tools/list; if [[ $? -eq 0 ]]; then INSPECTOR_SUCCESS=true; fi; set -e
                   fi
                   if ! $INSPECTOR_SUCCESS && [ -f "app.py" ]; then
                     echo "--> Trying '$PYTHON_EXEC app.py'..."
                     SERVER_CMD="$PYTHON_EXEC app.py"
                     set +e; timeout 180s npx --yes @modelcontextprotocol/inspector --cli "$SERVER_CMD" --method tools/list; if [[ $? -eq 0 ]]; then INSPECTOR_SUCCESS=true; fi; set -e
                   fi
                   if ! $INSPECTOR_SUCCESS && [ -f "server.py" ]; then
                     echo "--> Trying '$PYTHON_EXEC server.py'..."
                     SERVER_CMD="$PYTHON_EXEC server.py"
                     set +e; timeout 180s npx --yes @modelcontextprotocol/inspector --cli "$SERVER_CMD" --method tools/list; if [[ $? -eq 0 ]]; then INSPECTOR_SUCCESS=true; fi; set -e
                   fi
                   # Try running as module if others fail
                   if ! $INSPECTOR_SUCCESS ; then
                     PY_MODULE_NAME=$(basename "$PWD" | sed 's/-/_/g') # Convert dashes for module name
                     # Check if the module path actually exists (as dir or .py file)
                     if [ -d "$PY_MODULE_NAME" ] || [ -f "${PY_MODULE_NAME}.py" ]; then
                       echo "--> Trying '$PYTHON_EXEC -m $PY_MODULE_NAME'..."
                       SERVER_CMD="$PYTHON_EXEC -m $PY_MODULE_NAME"
                       set +e; timeout 180s npx --yes @modelcontextprotocol/inspector --cli "$SERVER_CMD" --method tools/list; if [[ $? -eq 0 ]]; then INSPECTOR_SUCCESS=true; fi; set -e
                     else
                        echo "--> Module '$PY_MODULE_NAME' not found, skipping module execution test."
                     fi
                   fi
                   # Final status based on inspector result
                   if $INSPECTOR_SUCCESS; then
                     SUB_STATUS="SUCCESS"; SUB_REASON="Tested via Python run (stdio using '$SERVER_CMD')";
                   else
                     echo "::error::Inspector failed with standard Python commands.";
                     SUB_STATUS="FAILURE"; SUB_REASON="Inspector failed with standard Python commands."; exit 1; # Fail subshell
                   fi
                   deactivate || true # Deactivate venv

                 # Priority 6: go.mod (Go)
                 elif [ -f "go.mod" ]; then
                   SUB_RUN_METHOD="Go (go.mod)"
                   echo "--> Found go.mod. Running go build..."
                   BINARY_NAME="mcp-server-go-binary"
                   timeout 300s go build -o "$BINARY_NAME" . # Build errors handled by set -e
                   # Check if build actually produced the binary
                   [ -f "$BINARY_NAME" ] || { echo "::error::Go build failed to produce binary '$BINARY_NAME'"; exit 1; } # Fail subshell if binary missing
                   chmod +x "$BINARY_NAME" # Ensure it's executable
                   # Try running with stdio argument first, then without
                   SERVER_CMD=""
                   INSPECTOR_SUCCESS=false
                   echo "--> Trying './$BINARY_NAME stdio'..."
                   SERVER_CMD="./$BINARY_NAME stdio"
                   set +e; timeout 180s npx --yes @modelcontextprotocol/inspector --cli "$SERVER_CMD" --method tools/list; if [[ $? -eq 0 ]]; then INSPECTOR_SUCCESS=true; fi; set -e
                   # If first attempt failed, try without 'stdio' argument
                   if ! $INSPECTOR_SUCCESS ; then
                     echo "--> Trying './$BINARY_NAME'..."
                     SERVER_CMD="./$BINARY_NAME"
                     set +e; timeout 180s npx --yes @modelcontextprotocol/inspector --cli "$SERVER_CMD" --method tools/list; if [[ $? -eq 0 ]]; then INSPECTOR_SUCCESS=true; fi; set -e
                   fi
                   # Final status based on inspector result
                   if $INSPECTOR_SUCCESS; then
                     SUB_STATUS="SUCCESS"; SUB_REASON="Tested via Go build/run (stdio using '$SERVER_CMD')";
                   else
                     echo "::error::Inspector failed with standard Go commands ('./$BINARY_NAME stdio' or './$BINARY_NAME').";
                     SUB_STATUS="FAILURE"; SUB_REASON="Inspector failed with standard Go commands."; exit 1; # Fail subshell
                   fi

                 # Priority 7: No Method Found
                 else
                   SUB_RUN_METHOD="None"
                   SUB_STATUS="SKIPPED"
                   SUB_REASON="No specific method known and no standard build/package config found (Dockerfile, package.json, pyproject.toml, requirements.txt, go.mod)."
                   echo "--> No recognized project type found."; exit 0; # Graceful exit for skip
                 fi

                 # ==========================================
                 # ===== END OF TEST LOGIC BLOCK ==========
                 # ==========================================

                 # If script reaches here without exiting, it implies success (or the last command succeeded)
                 echo "--- Subshell test logic appears successful ---"
                 # Explicitly clear the trap and write the final result before exiting normally
                 trap - EXIT
                 write_result "$SUB_STATUS" "$SUB_REASON" "$SUB_RUN_METHOD"
                 exit 0 # Explicit success exit

               ) > "$SUBSHELL_LOG_FILE" 2>&1; SUBSHELL_EXIT_CODE=$? # Redirect subshell output, capture exit code

               # --- Process Subshell Results ---
               echo "--> Subshell finished (Exit Code: $SUBSHELL_EXIT_CODE)"
               if [ -f "$SUBSHELL_RESULT_FILE" ]; then
                  source "$SUBSHELL_RESULT_FILE" # Load SUB_STATUS, SUB_REASON, SUB_RUN_METHOD
                  STATUS=$SUB_STATUS
                  REASON=$SUB_REASON
                  RUN_METHOD=$SUB_RUN_METHOD
                  # If subshell exited non-zero, it means a failure occurred (due to set -e or explicit exit 1)
                  # Override status to FAILURE if it wasn't already set by the trap or logic.
                  if [[ $SUBSHELL_EXIT_CODE -ne 0 && "$STATUS" != "FAILURE" ]]; then
                     echo "::warning:: Subshell exited non-zero ($SUBSHELL_EXIT_CODE) but result file indicates '$STATUS'. Overriding to FAILURE."
                     STATUS="FAILURE"
                     # Append exit code info to reason if possible
                     REASON="$REASON (Subshell exited $SUBSHELL_EXIT_CODE)"
                  fi
               else
                 # Result file missing indicates a major problem
                 echo "::error:: Subshell result file missing! Subshell likely crashed before trap could write results."
                 STATUS="FAILURE"
                 REASON="Subshell execution failed catastrophically (Exit code: $SUBSHELL_EXIT_CODE, result file missing)."
                 RUN_METHOD="Unknown"
               fi

               # Log subshell output, especially on failure for easier debugging
               if [[ "$STATUS" == "FAILURE" ]]; then
                 echo "--- Subshell Log (Failure) ---"
                 cat "$SUBSHELL_LOG_FILE" || echo "INFO: Could not display subshell log file $SUBSHELL_LOG_FILE."
                 echo "--- End Subshell Log ---"
               fi
               # Clean up subshell temporary files
               rm -f "$SUBSHELL_LOG_FILE" "$SUBSHELL_RESULT_FILE"
            fi # End of checkout success check

            # --- C. Reporting for this iteration ---
            TEST_END_TIME=$(date +%s); DURATION=$((TEST_END_TIME - TEST_START_TIME))
            echo ""
            echo "--- Test Result [$CURRENT_INDEX/$REPO_COUNT] :: ${REPO} ---"
            echo "- Run Method Attempted: ${RUN_METHOD}"
            echo "- Outcome: ${STATUS}"
            echo "- Details: ${REASON}"
            echo "- Duration: ${DURATION}s"
            echo "--------------------------------------------------"
            echo ""
            # Store result detail using jq (ensure jq installed)
            command -v jq >/dev/null 2>&1 || { echo "::error:: 'jq' command not found!"; exit 1; }
            RESULT_JSON=$(jq -nc --arg repo "$REPO" --arg status "$STATUS" --arg reason "$REASON" --arg method "$RUN_METHOD" --argjson duration "$DURATION" \
              '{repo: $repo, status: $status, reason: $reason, method: $method, duration: $duration}')
            RESULTS+=("$RESULT_JSON") # Add JSON object to bash array

            # Increment counters for summary
            case $STATUS in SUCCESS) SUCCESS_COUNT=$((SUCCESS_COUNT + 1));; FAILURE) FAILURE_COUNT=$((FAILURE_COUNT + 1));; SKIPPED) SKIPPED_COUNT=$((SKIPPED_COUNT + 1));; esac

            # Use annotations for better visibility in Actions UI
            ANNOTATION_REASON=$(echo "$REASON" | sed 's/%/%25/g; s/\r/%0D/g; s/\n/%0A/g') # Escape for annotation
            if [[ "$STATUS" == "FAILURE" ]]; then
              echo "::error title=Test Failed [$CURRENT_INDEX/$REPO_COUNT] ${REPO}::Repo=${REPO}, Status=${STATUS}, Method=${RUN_METHOD}, Reason=${ANNOTATION_REASON}"
            elif [[ "$STATUS" == "SKIPPED" ]]; then
              echo "::warning title=Test Skipped [$CURRENT_INDEX/$REPO_COUNT] ${REPO}::Repo=${REPO}, Status=${STATUS}, Method=${RUN_METHOD}, Reason=${ANNOTATION_REASON}"
            else # SUCCESS
               echo "::notice title=Test Passed [$CURRENT_INDEX/$REPO_COUNT] ${REPO}::Repo=${REPO}, Status=${STATUS}, Method=${RUN_METHOD}, Reason=${ANNOTATION_REASON}"
            fi

            # --- D. Cleanup for next iteration ---
            echo "--> Cleaning up checkout directory ${CHECKOUT_PATH}..."
            rm -rf "$CHECKOUT_PATH" || echo "::warning:: Failed to remove checkout directory $CHECKOUT_PATH"
          done < "$REPO_LIST_FILE" # Feed the loop from the repo list file


          # --- E. Final Summary ---

          echo ""

          echo "======================================================================"

          echo "Test Run Summary"

          echo "======================================================================"

          echo "Total Servers Processed: $REPO_COUNT"

          echo "Success: $SUCCESS_COUNT"; echo "Failure: $FAILURE_COUNT"; echo "Skipped: $SKIPPED_COUNT"

          echo "======================================================================"

          # Generate detailed results JSON file from the RESULTS array

          echo "Generating detailed results JSON..."

          printf "%s\n" "${RESULTS[@]}" | jq -s . > detailed_results.json

          echo "Detailed results saved to detailed_results.json"


          # Exit with error code 1 if any failures occurred, otherwise exit 0

          if [ "$FAILURE_COUNT" -gt 0 ]; then
            echo "::error::One or more server tests reported a FAILURE status!"
            exit 1
          else
            echo "All server tests completed. Status: $SUCCESS_COUNT Success, $FAILURE_COUNT Failure, $SKIPPED_COUNT Skipped."
            exit 0
          fi
      - name: Upload Detailed Results Artifact
        if: success() || failure()
        uses: actions/upload-artifact@v4
        with:
          name: detailed-test-results
          path: detailed_results.json
          if-no-files-found: warn
      - name: Final Status Report
        if: always()
        run: >
          echo "Job finished with status: ${{ job.status }}"

          # The job's success/failure is automatically determined by the exit codes of previous steps.

          # Specifically, if Step 3 exits with 1, the job will be marked as failed.
